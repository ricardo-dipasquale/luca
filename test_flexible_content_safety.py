#!/usr/bin/env python3
"""
Test de Content Safety Flexible - Moderation API vs LLM

Este script prueba que el sistema puede alternar entre usar OpenAI Moderation API
y GPT-4o-mini para content safety seg√∫n la configuraci√≥n.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))


async def test_llm_method():
    """Test content safety using LLM method."""
    print("ü§ñ === TEST: Content Safety con LLM (GPT-4o-mini) ===")
    
    from guardrails import EducationalGuardrailLayer, GuardrailConfig, EducationalContext
    
    # Force LLM method
    config = GuardrailConfig(
        content_safety_method="llm",
        enable_openai_moderation=False,  # This will be overridden by method
        enable_profanity_filter=True,
        enable_educational_validation=False,  # Focus on content safety
        enable_rate_limiting=False,
        enable_langfuse_logging=False
    )
    
    guardrail = EducationalGuardrailLayer(config)
    context = EducationalContext(
        student_id="test_student",
        session_id="test_session",
        subject="Bases de Datos Relacionales"
    )
    
    test_cases = [
        ("¬øQu√© es un LEFT JOIN?", "‚úÖ Debe pasar - consulta acad√©mica"),
        ("te hago una pregunta boluda: el ejercicio 1.a es f√°cil?", "‚ùì LLM debe evaluar contexto"),
        ("Esto es una mierda total", "‚ùå Debe bloquear - profanidad clara"),
        ("Che, no entiendo el JOIN", "‚ùì LLM debe considerar contexto argentino")
    ]
    
    for test_input, expected in test_cases:
        print(f"\nüìù Input: '{test_input}'")
        print(f"üéØ Esperado: {expected}")
        
        try:
            result = await guardrail.validate_input(test_input, context)
            
            if result.passed:
                print("‚úÖ APROBADO por LLM")
            else:
                print("‚ùå BLOQUEADO por LLM")
                for violation in result.violations:
                    print(f"   - {violation.message}")
                    if violation.details.get("method") == "llm_gpt4o_mini":
                        print(f"   - Raz√≥n LLM: {violation.details.get('reason')}")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")


async def test_environment_configuration():
    """Test that environment variables control the method selection."""
    print("\nüåç === TEST: Configuraci√≥n por Variables de Entorno ===")
    
    # Set environment variable temporarily
    original_method = os.environ.get('GUARDRAILS_CONTENT_SAFETY_METHOD')
    os.environ['GUARDRAILS_CONTENT_SAFETY_METHOD'] = 'llm'
    
    try:
        from guardrails.config import load_guardrail_config_from_env
        
        config = load_guardrail_config_from_env()
        
        print(f"üìä M√©todo configurado: {config.content_safety_method}")
        print(f"üîß OpenAI Moderation habilitado: {config.enable_openai_moderation}")
        print(f"üõ°Ô∏è Filtro de profanidad habilitado: {config.enable_profanity_filter}")
        
        if config.content_safety_method == "llm":
            print("‚úÖ Configuraci√≥n correcta: Usando LLM para content safety")
        else:
            print("‚ùå Error: Configuraci√≥n no refleja la variable de entorno")
            
    finally:
        # Restore original environment
        if original_method is not None:
            os.environ['GUARDRAILS_CONTENT_SAFETY_METHOD'] = original_method
        else:
            os.environ.pop('GUARDRAILS_CONTENT_SAFETY_METHOD', None)


async def test_orchestrator_integration():
    """Test that orchestrator uses the new flexible configuration."""
    print("\nüé≠ === TEST: Integraci√≥n Orchestrator con Configuraci√≥n Flexible ===")
    
    try:
        from orchestrator.agent_executor import OrchestratorAgentExecutor
        
        # Initialize orchestrator (should read from environment)
        executor = OrchestratorAgentExecutor(enable_guardrails=True)
        
        # Check configuration
        status = executor.get_guardrail_status("test_student")
        
        if status.get('guardrails_enabled'):
            config = status.get('config', {})
            method = config.get('content_safety_method', 'unknown')
            
            print(f"‚úÖ Orchestrator inicializado con guardrails")
            print(f"üìä M√©todo de content safety: {method}")
            print(f"üîß Configuraci√≥n desde variables de entorno: {method == 'llm'}")
        else:
            print("‚ùå Guardrails no habilitados en Orchestrator")
            
    except Exception as e:
        print(f"‚ùå Error en integraci√≥n Orchestrator: {e}")


async def main():
    """Run all tests."""
    print("üß™ === TEST COMPLETO: Sistema de Content Safety Flexible ===")
    print("Verificando que el sistema puede usar tanto Moderation API como LLM")
    print()
    
    await test_llm_method()
    await test_environment_configuration() 
    await test_orchestrator_integration()
    
    print("\nüéâ === TESTS COMPLETADOS ===")
    print("Verificar que:")
    print("1. ‚úÖ LLM method funciona correctamente")
    print("2. ‚úÖ Variables de entorno controlan la configuraci√≥n") 
    print("3. ‚úÖ Orchestrator usa la configuraci√≥n flexible")


if __name__ == "__main__":
    asyncio.run(main())