# LUCA Agent Testing Framework

Sistema completo de testing para agentes educativos de LUCA con integraciÃ³n a Langfuse.

## ğŸš€ CaracterÃ­sticas

- **GestiÃ³n de Suites**: Crear y administrar suites de pruebas en JSON
- **EjecuciÃ³n Multi-Agente**: Testear Orchestrator y GapAnalyzer 
- **MÃ©tricas Avanzadas**: RecolecciÃ³n automÃ¡tica de mÃ©tricas de agentes
- **IntegraciÃ³n Langfuse**: Subida de datasets y tracking de ejecuciones
- **CLI Completo**: Interfaz de lÃ­nea de comandos intuitiva
- **Reportes y AnÃ¡lisis**: GeneraciÃ³n de reportes y anÃ¡lisis de tendencias

## ğŸ“¦ InstalaciÃ³n

```bash
# Navegar al directorio del proyecto
cd /home/ricardo/git/luca

# Instalar dependencias adicionales
pip install langfuse click pydantic

# Verificar instalaciÃ³n
python -m agent-test.cli --help
```

## ğŸ› ï¸ ConfiguraciÃ³n

### Variables de Entorno

```bash
# Langfuse (opcional)
export LANGFUSE_HOST="http://localhost:3000"
export LANGFUSE_PUBLIC_KEY="pk-lf-your-key"
export LANGFUSE_SECRET_KEY="sk-lf-your-key"

# Neo4j (ya configurado)
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="your_password"

# OpenAI (ya configurado)
export OPENAI_API_KEY="your_openai_key"
```

## ğŸ“š Uso BÃ¡sico

### 1. GestiÃ³n de Suites

```bash
# Crear nueva suite
python -m agent-test.cli suite create mi_suite --agent=orchestrator --description="Suite de prueba"

# Agregar pregunta
python -m agent-test.cli suite add-question mi_suite \
  --question="Â¿QuÃ© es un LEFT JOIN?" \
  --expected="ExplicaciÃ³n de LEFT JOIN con ejemplos" \
  --subject="Bases de Datos" \
  --difficulty=medium

# Listar suites
python -m agent-test.cli suite list

# Ver detalles de suite
python -m agent-test.cli suite show mi_suite
```

### 2. IntegraciÃ³n con Langfuse

```bash
# Subir suite como dataset
python -m agent-test.cli dataset upload mi_suite --name="Suite BÃ¡sica BD"

# Listar datasets en Langfuse
python -m agent-test.cli dataset list
```

### 3. EjecuciÃ³n de Pruebas

```bash
# Ejecutar suite individual
python -m agent-test.cli run mi_suite --iterations=1

# Ejecutar con agente especÃ­fico
python -m agent-test.cli run mi_suite --agent=orchestrator

# Ejecutar todas las suites
python -m agent-test.cli run-all
```

### 4. AnÃ¡lisis de Resultados

```bash
# Listar ejecuciones recientes
python -m agent-test.cli results list

# Ver detalles de ejecuciÃ³n
python -m agent-test.cli results show <run_id>

# Filtrar por suite
python -m agent-test.cli results list --suite=mi_suite --limit=5
```

## ğŸ“Š Suites de Ejemplo

El sistema incluye suites de ejemplo:

### Orchestrator BÃ¡sico
```bash
python -m agent-test.cli suite show orchestrator_basic_qa
python -m agent-test.cli run orchestrator_basic_qa
```

### GapAnalyzer PrÃ¡cticas
```bash
python -m agent-test.cli suite show gapanalyzer_practice_analysis
python -m agent-test.cli run gapanalyzer_practice_analysis
```

## ğŸ“ˆ MÃ©tricas Recolectadas

### Orchestrator Agent
- **Intent Detection**: Intents detectados y confianza
- **Routing Decisions**: Decisiones de enrutamiento a GapAnalyzer
- **Knowledge Graph Usage**: Consultas y resultados del KG
- **Response Quality**: Longitud, ejemplos, notaciÃ³n matemÃ¡tica
- **Educational Content**: Tipo de explicaciÃ³n, profundidad conceptual

### GapAnalyzer Agent
- **Gap Analysis**: Gaps identificados y tipos
- **Content Retrieval**: Contenido relevante encontrado
- **Pedagogical Elements**: Scaffolding, hints, sugerencias
- **Explanation Depth**: Nivel de profundidad en explicaciones

### MÃ©tricas Generales
- **Performance**: Tiempo de ejecuciÃ³n, tasa de Ã©xito
- **Response Quality**: Completitud, claridad, relevancia
- **Educational Value**: Valor pedagÃ³gico de las respuestas

## ğŸ—ï¸ Estructura del Proyecto

```
agent-test/
â”œâ”€â”€ cli.py                    # CLI principal
â”œâ”€â”€ __main__.py              # Entry point
â”œâ”€â”€ schemas.py               # Esquemas Pydantic
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml         # ConfiguraciÃ³n
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ suite_manager.py    # GestiÃ³n de suites
â”‚   â”œâ”€â”€ test_runner.py      # Ejecutor de pruebas
â”‚   â”œâ”€â”€ metrics_collector.py # RecolecciÃ³n de mÃ©tricas  
â”‚   â”œâ”€â”€ results_manager.py  # GestiÃ³n de resultados
â”‚   â””â”€â”€ langfuse_integration.py # IntegraciÃ³n Langfuse
â”œâ”€â”€ suites/                 # Archivos de suites JSON
â”œâ”€â”€ results/                # Resultados de ejecuciones
â”‚   â”œâ”€â”€ runs/              # Resultados individuales
â”‚   â”œâ”€â”€ summaries/         # ResÃºmenes por suite
â”‚   â””â”€â”€ exports/           # Exportaciones
â””â”€â”€ README.md
```

## ğŸ”§ Comandos Avanzados

### ValidaciÃ³n de Suites
```bash
# Validar suite antes de ejecuciÃ³n
python -c "
from agent_test.core.suite_manager import SuiteManager
manager = SuiteManager()
result = manager.validate_suite('mi_suite')
print(result)
"
```

### ExportaciÃ³n de Resultados
```bash
# Exportar mÃºltiples runs a CSV
python -c "
from agent_test.core.results_manager import ResultsManager
manager = ResultsManager()
runs = manager.list_runs(limit=5)
run_ids = [r['run_id'] for r in runs]
export_path = manager.export_results(run_ids, format='csv')
print(f'Exportado a: {export_path}')
"
```

### AnÃ¡lisis de Tendencias
```bash
# Analizar tendencias de performance
python -c "
from agent_test.core.results_manager import ResultsManager
manager = ResultsManager()
trends = manager.get_performance_trends('mi_suite', days=7)
print(trends)
"
```

## ğŸ› Troubleshooting

### Error de ImportaciÃ³n
```bash
# Verificar que estÃ¡s en el directorio correcto
pwd  # DeberÃ­a ser /home/ricardo/git/luca

# Verificar estructura de archivos
ls -la agent-test/
```

### Error de Langfuse
```bash
# Verificar conexiÃ³n
python -c "
from agent_test.core.langfuse_integration import check_langfuse_availability
print('Langfuse disponible:', check_langfuse_availability())
"
```

### Error de Agentes
```bash
# Testear agentes individualmente
python -m orchestrator.local_runner single "Test message"
python -m gapanalyzer.local_runner 1 1.a "Test query"
```

## ğŸ¤ Desarrollo

### Agregar Nuevas MÃ©tricas

1. Editar `schemas.py` para agregar campos a `OrchestratorMetrics` o `GapAnalyzerMetrics`
2. Implementar recolecciÃ³n en `metrics_collector.py`
3. Actualizar patrones de detecciÃ³n si es necesario

### Agregar Nuevos Tipos de Agente

1. Agregar al enum `AgentType` en `schemas.py`
2. Implementar ejecuciÃ³n en `test_runner.py`
3. Agregar mÃ©tricas especÃ­ficas en `metrics_collector.py`

### Nuevos Formatos de ExportaciÃ³n

1. Editar `results_manager.py` mÃ©todo `export_results()`
2. Implementar nueva funciÃ³n de exportaciÃ³n
3. Actualizar CLI para soportar el nuevo formato

## ğŸ“ TODO / Mejoras Futuras

- [ ] Interfaz web para visualizar resultados
- [ ] Evaluaciones automÃ¡ticas con LLM
- [ ] ComparaciÃ³n automÃ¡tica con respuestas esperadas
- [ ] Alertas automÃ¡ticas por degradaciÃ³n de performance
- [ ] IntegraciÃ³n con CI/CD para testing continuo
- [ ] Dashboard de mÃ©tricas en tiempo real
- [ ] Soporte para mÃºltiples proyectos/equipos

## ğŸ“„ Licencia

Parte del proyecto LUCA - Sistema educativo con IA.